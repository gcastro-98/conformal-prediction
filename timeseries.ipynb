{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This work was highly based on this [notebook](https://github.com/scikit-learn-contrib/MAPIE/blob/master/notebooks/regression/ts-changepoint.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time series problem\n",
    "Within this notebook, the \"EnbPI\" strategy ([Xu, Chen and Yao Xie (2021)](https://proceedings.mlr.press/v139/xu21h.html) ${}^{[1]}$) is implemented to estimate prediction intervals for time series' forecasts. In particular, the Victoria electricity demand dataset is used (as in the book: \"Forecasting: Principles and Practice\" by R. J. Hyndman and G. Athanasopoulos).\n",
    "\n",
    "The electricity demand features daily and weekly seasonalities and is impacted\n",
    "by the temperature, considered here as a exogeneous variable. Thus, the features for this prediction problem will be:\n",
    "- The lagged `n_lag` timestamps\n",
    "- The week of year\n",
    "- The weekday \n",
    "- The hour of the day\n",
    "- The temperature \n",
    "\n",
    "A Random Forest model will be used to fit the data (fine-tuning it using a randomized hyperparameters search, leveraging a sequential [`sklearn.model_selection.TimeSeriesSplit`](sklearn.model_selection.TimeSeriesSplit) cross validation), and then 'EnbPI' will be implemented through the [`mapie.time_series_regression.MapieTimeSeriesRegressor`](https://mapie.readthedocs.io/en/latest/generated/mapie.regression.MapieTimeSeriesRegressor.html) class.\n",
    "\n",
    "${}^{[1]}$ Xu, Chen and Yao Xie (July 2021). “Conformal prediction interval for dynamic time-series”. In: Proceedings of the 38th International Conference on Machine Learning. Ed. Bibliography 29 by Marina Meila and Tong Zhang. Vol. 139. Proceedings of Machine Learning Research. PMLR, pp. 11559–11569. URL: https://proceedings.mlr.press/v139/xu21h.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compare 2 strategies to quantify the uncertainty: applying 'EnbPI' with or without ``partial_fit`` called at every step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle  # to serialize the results of the coverage cross-validation\n",
    "import json  # to serialize the best hyperparameters found\n",
    "import warnings  # to suppress them\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from cp import data, visualize, validate, ts, cv, logger as _logger\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "SEED: int = 123\n",
    "np.random.seed(SEED)\n",
    "MISCOVERAGE: float = 0.2 # MISCOVERAGE = alpha # CONFIDENCE = 1-alpha \n",
    "logger = _logger.Logger()\n",
    "warnings.filterwarnings(\"ignore\")  # to suppress them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_problem = data.TimeSeriesProblem()\n",
    "X_train, X_test, y_train, y_test = ts_problem.get_arrays()\n",
    "ts_problem.visualize_data();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define perform a random search to find out which are the best hyperparameters for our random forest regressor (our base model):\n",
    "- **Note:** it may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_serialization_file: str = \"output/rf-hyperparameters.json\"\n",
    "\n",
    "if os.path.isfile(hyperparameters_serialization_file):\n",
    "    logger.info(\"Retrieving best hyperparameters from local file\")\n",
    "    with open(hyperparameters_serialization_file, \"r\") as json_file:\n",
    "        best_model_params: dict = json.load(json_file)\n",
    "else:\n",
    "    best_model_params = cv.fine_tune_rf_for_ts(X_train, y_train)\n",
    "    with open(hyperparameters_serialization_file, \"w\") as json_file:\n",
    "        json.dump(best_model_params, json_file, indent=4)\n",
    "\n",
    "logger.info(f\"The best hyperparameters found were: {best_model_params}\")\n",
    "best_model_params.update({'random_state': SEED, 'verbose': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy_name: dict = {\n",
    "    'EnbPI_nP': 'Ensemble of bootstrap Prediction Intervals (w/o partial fit)',\n",
    "    'EnbPI': 'Ensemble of bootstrap Prediction Intervals'\n",
    "}\n",
    "base_model: dict = {\n",
    "    'EnbPI_nP': RandomForestRegressor(**best_model_params),\n",
    "    'EnbPI': RandomForestRegressor(**best_model_params)\n",
    "}\n",
    "y_pred, int_pred, mapie_estimator = {}, {}, {}\n",
    "\n",
    "y_pred['EnbPI_nP'], int_pred['EnbPI_nP'], mapie_estimator['EnbPI_nP'] = ts.train_without_partial_fit(\n",
    "    X_train, y_train, X_test, MISCOVERAGE, \n",
    "    RandomForestRegressor(**best_model_params)\n",
    ")\n",
    "y_pred['EnbPI'], int_pred['EnbPI'], mapie_estimator['EnbPI'] = ts.train(\n",
    "    X_train, y_train, X_test, y_test, MISCOVERAGE, \n",
    "    RandomForestRegressor(**best_model_params)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverages: dict = validate.coverage(int_pred, y_test)\n",
    "widths: dict = validate.width(int_pred)\n",
    "rmse: dict = validate.rmse(y_pred, y_test)\n",
    "cwc: dict = validate.cwc(int_pred, y_test, MISCOVERAGE, eta=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first visualize the predicted values and intervals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_figs: int = len(base_model)\n",
    "fig, axs = plt.subplots(nrows=n_figs, figsize=(10, 5 * n_figs));\n",
    "\n",
    "for _i, _strat in enumerate(strategy_name.keys()):\n",
    "    # Predicted intervals\n",
    "    _y_pred_low = int_pred[_strat][:, 0, 0].ravel()\n",
    "    _y_pred_up = int_pred[_strat][:, 1, 0].ravel()\n",
    "\n",
    "    # Visualize\n",
    "    axs[_i] = visualize.ts(\n",
    "        points={\n",
    "            'X_train': ts_problem.train_df.index.values, \n",
    "            'X': ts_problem.test_df.index.values, \n",
    "            'y_train': y_train,\n",
    "            'y': y_test,\n",
    "            'y_pred': y_pred[_strat],\n",
    "            },\n",
    "        intervals={\n",
    "            'X': ts_problem.test_df.index.values, \n",
    "            'y_low': _y_pred_low, \n",
    "            'y_up': _y_pred_up\n",
    "            },\n",
    "        title=strategy_name[_strat], \n",
    "        ax=axs[_i]\n",
    "        )\n",
    "    \n",
    "plt.savefig('output/prediction-intervals-timeseries-problem.png', dpi=200)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us assess some metrics and plot different validation figures:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results in function of time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Marginal coverage on 24h rolling windows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE: int = 24  # in hours \n",
    "rolling_coverage = {_k: [] for _k in strategy_name.keys()}\n",
    "\n",
    "for i in range(WINDOW_SIZE, len(y_test)):\n",
    "    for _strat in strategy_name.keys():\n",
    "        rolling_coverage[_strat].append(\n",
    "            validate.coverage(\n",
    "                {_strat: int_pred[_strat][i-WINDOW_SIZE:i, :, :]}, \n",
    "                y_test[i-WINDOW_SIZE:i], silent=True)[_strat])\n",
    "\n",
    "visualize.rolling_coverage(\n",
    "    rolling_coverage, ts_problem.test_df[WINDOW_SIZE:].index, \n",
    "    WINDOW_SIZE, save_path='output/rolling-coverage.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results aggregated in time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Average goodness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_figs = len(base_model)\n",
    "fig, axs = plt.subplots(nrows=n_figs, figsize=(10, 5 * n_figs));\n",
    "\n",
    "for _i, _strat in enumerate(strategy_name.keys()):\n",
    "    # Predicted intervals\n",
    "    _y_pred_low = int_pred[_strat][:, 0, 0].ravel()\n",
    "    _y_pred_up = int_pred[_strat][:, 1, 0].ravel()\n",
    "\n",
    "    # Visualize\n",
    "    axs[_i] = visualize.goodness(\n",
    "        y_test, y_pred[_strat],\n",
    "        _y_pred_low, \n",
    "        _y_pred_up,\n",
    "        coverages[_strat],\n",
    "        widths[_strat],\n",
    "        rmse[_strat],\n",
    "        cwc[_strat],\n",
    "        ax=axs[_i],\n",
    "        fading_with_lead_time=True if _strat == 'EnbPI_nP' else False,\n",
    "        xy=(2.7, 6.5),\n",
    "        xytext=(2.7, 5.95),\n",
    "        subsample=0.5,\n",
    "        title=strategy_name[_strat], \n",
    "    )\n",
    "plt.savefig('output/average-goodness-timeseries-problem.png', dpi=200)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For visualization purposes, note that just a 50% of the test data was shown. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Marginal vs. conditional coverage: **width occurrence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_figs = len(base_model)\n",
    "fig, axs = plt.subplots(nrows=n_figs, figsize=(10, 5 * n_figs));\n",
    "\n",
    "_x_max = (1+1e-3) * np.max([np.abs(int_pred[_strat][:, 0, 0] - int_pred[_strat][:, 1, 0]) for _strat in int_pred.keys()]) \n",
    "_x_min = np.min([np.abs(int_pred[_strat][:, 0, 0] - int_pred[_strat][:, 1, 0]) for _strat in int_pred.keys()])\n",
    "\n",
    "for _i, _strat in enumerate(base_model.keys()):\n",
    "    # Visualize\n",
    "    axs[_i] = visualize.width_size_occurrence(\n",
    "        int_pred[_strat],\n",
    "        # train_intervals=int_pred_train[_strat],\n",
    "        num_bins=5,\n",
    "        ax=axs[_i],\n",
    "        x_lim=[_x_min, _x_max],\n",
    "        title=strategy_name[_strat], \n",
    "    )\n",
    "plt.savefig('output/width-occurrence-timeseries-problem.png', dpi=200)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Marginal vs. conditional coverage: **coverage vs. width**\n",
    "    - Since `EnbPI_nP` provides constant-width intervals, its plot is omitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_BINS: int = 10\n",
    "_strategies = [_k for _k in base_model.keys() if _k != 'EnbPI_nP']\n",
    "\n",
    "n_figs = len(_strategies)\n",
    "fig, axs = plt.subplots(nrows=n_figs, figsize=(10, 5 * n_figs));\n",
    "if not hasattr(axs, '__len__'):  # because nrows=1\n",
    "    axs = [axs]\n",
    "    \n",
    "cond_coverages: dict = validate.cond_coverage(int_pred, y_test, num_bins=NUM_BINS)\n",
    "hsic_coefficients: dict = validate.hsic_coefficient(int_pred, y_test) \n",
    "\n",
    "for _i, _strat in enumerate(_strategies):\n",
    "    axs[_i] = visualize.coverage_by_width(\n",
    "        y_test, int_pred[_strat], MISCOVERAGE,\n",
    "        cond_coverages[_strat],\n",
    "        hsic_coefficients[_strat],\n",
    "        num_bins=NUM_BINS,\n",
    "        ax=axs[_i],\n",
    "        title=strategy_name[_strat], \n",
    "    )\n",
    "plt.savefig('output/coverage-vs-width-timeseries-problem.png', dpi=200)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more detailed interpretation of SSC score and the HSIC coefficient, one can refer to this [page](https://mapie.readthedocs.io/en/stable/examples_regression/2-advanced-analysis/plot_conditional_coverage.html). Below a list with the all metrics shown so far is represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = visualize.dicts_to_dataframe(\n",
    "    {'Coverage': coverages, 'Int. width': widths, \n",
    "     'RMSE': rmse, 'CWC': cwc, \n",
    "     'SSC score': cond_coverages, \n",
    "     'HSIC': hsic_coefficients})\n",
    "\n",
    "visualize.dataframe_to_png(metrics_df, 'output/metrics-table-timeseries-problem.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Coverage sougth for different values of $\\alpha$**: and, finally, a visualization which displays the performance of each strategy when $\\alpha$ is changed\n",
    "    - Note in this case the K-fold cross-validation can not be randomly done with shuffling; instead, we need to take continuous splits of the dataset to break as much as temporal autocorrelation as possible. Actually, below a plot using a 5-fold validation is provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.visualize_ts_K_folds(5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Coverage sougth for different values of $\\alpha$**\n",
    "    - Below, the 5-fold CV is executed; and later its results, plotted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_RESULTS: str = 'output/coverage-cv-results.pickle'\n",
    "MISCOVERAGES_LIST: np.ndarray = np.round(np.linspace(0.01, 0.20, 5), 2)\n",
    "N_FOLDS: int = 5\n",
    "\n",
    "if os.path.isfile(CV_RESULTS):\n",
    "    logger.info(\"Opening serialized CV results\")\n",
    "    with open(CV_RESULTS, 'rb') as f:\n",
    "        coverages_dict = pickle.load(f)\n",
    "else:\n",
    "    coverages_dict = {}\n",
    "    logger.info(\"Training the different strategis and for different alpha values\")\n",
    "    logger.debug(4 * \" \" + \"This may take a while (AROUND 30' MINUTES)\")\n",
    "\n",
    "    for _strat, base_estimator in base_model.items():\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            coverages_dict[_strat] = cv.ts_coverage_in_function_of_alpha(\n",
    "                MISCOVERAGES_LIST, best_model_params, _strat, silent=True, K=N_FOLDS)\n",
    "    \n",
    "    logger.info(\"Serializing obtained CV results\")\n",
    "    with open(CV_RESULTS, 'wb') as f:\n",
    "        pickle.dump(coverages_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can proceed to plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_figs = len(base_model)\n",
    "fig, axs = plt.subplots(nrows=n_figs, figsize=(10, 5 * n_figs));\n",
    "\n",
    "for _i, _strat in enumerate(base_model.keys()):\n",
    "    axs[_i] = visualize.coverage_by_alpha(\n",
    "        coverages_dict[_strat], \n",
    "        MISCOVERAGES_LIST, \n",
    "        strategy_name[_strat],\n",
    "        ax=axs[_i]\n",
    "        )\n",
    "plt.savefig('output/coverage-vs-alpha-timeseries-problem.png', dpi=200)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced problem: test set contains a change point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle  # to serialize the results of the coverage cross-validation\n",
    "import json  # to serialize the best hyperparameters found\n",
    "import warnings  # to suppress them\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from cp import data, visualize, validate, ts, cv, logger as _logger\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "SEED: int = 123\n",
    "np.random.seed(SEED)\n",
    "MISCOVERAGE: float = 0.2 # MISCOVERAGE = alpha # CONFIDENCE = 1-alpha \n",
    "logger = _logger.Logger()\n",
    "warnings.filterwarnings(\"ignore\")  # to suppress them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now simulate the same situation as in the [original notebook](https://github.com/scikit-learn-contrib/MAPIE/blob/master/notebooks/regression/ts-changepoint.ipynb): that is, now the test set will contain a severe change point. To reproduce this, we will artificially decrease the electricity demand by 2 GW in such set (_e.g._ mocking off an exogeneous event such as a blackout or lockdown due to a pandemic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_problem = data.TimeSeriesProblem(with_change_point=True)\n",
    "X_train, X_test, y_train, y_test = ts_problem.get_arrays()\n",
    "ts_problem.visualize_data();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the former results of the RandomForest finetuning, since the training set did not change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_serialization_file: str = \"output/rf-hyperparameters.json\"\n",
    "\n",
    "logger.info(\"Retrieving best hyperparameters from local file\")\n",
    "with open(hyperparameters_serialization_file, \"r\") as json_file:\n",
    "    best_model_params: dict = json.load(json_file)\n",
    "\n",
    "logger.info(f\"The best hyperparameters found were: {best_model_params}\")\n",
    "best_model_params.update({'random_state': SEED, 'verbose': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we proceeded to train again the EnbPI (the version w/ partial fit should adapt to the change point) and carry out the inference. Also, its metrics are computed\n",
    "- To ease verbosity, the processes were silenced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy_name: dict = {\n",
    "    'EnbPI_nP': 'Ensemble of bootstrap Prediction Intervals (w/o partial fit)',\n",
    "    'EnbPI': 'Ensemble of bootstrap Prediction Intervals'\n",
    "}\n",
    "base_model: dict = {\n",
    "    'EnbPI_nP': RandomForestRegressor(**best_model_params),\n",
    "    'EnbPI': RandomForestRegressor(**best_model_params)\n",
    "}\n",
    "y_pred, int_pred, mapie_estimator = {}, {}, {}\n",
    "\n",
    "y_pred['EnbPI_nP'], int_pred['EnbPI_nP'], mapie_estimator['EnbPI_nP'] = ts.train_without_partial_fit(\n",
    "    X_train, y_train, X_test, MISCOVERAGE, \n",
    "    RandomForestRegressor(**best_model_params), silent=True)\n",
    "y_pred['EnbPI'], int_pred['EnbPI'], mapie_estimator['EnbPI'] = ts.train(\n",
    "    X_train, y_train, X_test, y_test, MISCOVERAGE, \n",
    "    RandomForestRegressor(**best_model_params), silent=True)\n",
    "\n",
    "coverages: dict = validate.coverage(int_pred, y_test, silent=True)\n",
    "widths: dict = validate.width(int_pred, silent=True)\n",
    "rmse: dict = validate.rmse(y_pred, y_test, silent=True)\n",
    "cwc: dict = validate.cwc(int_pred, y_test, MISCOVERAGE, eta=10, silent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see how the change point affected the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_figs: int = len(base_model)\n",
    "fig, axs = plt.subplots(nrows=n_figs, figsize=(10, 5 * n_figs));\n",
    "\n",
    "for _i, _strat in enumerate(strategy_name.keys()):\n",
    "    # Predicted intervals\n",
    "    _y_pred_low = int_pred[_strat][:, 0, 0].ravel()\n",
    "    _y_pred_up = int_pred[_strat][:, 1, 0].ravel()\n",
    "\n",
    "    # Visualize\n",
    "    axs[_i] = visualize.ts(\n",
    "        points={\n",
    "            'X_train': ts_problem.train_df.index.values, \n",
    "            'X': ts_problem.test_df.index.values, \n",
    "            'y_train': y_train,\n",
    "            'y': y_test,\n",
    "            'y_pred': y_pred[_strat],\n",
    "            },\n",
    "        intervals={\n",
    "            'X': ts_problem.test_df.index.values, \n",
    "            'y_low': _y_pred_low, \n",
    "            'y_up': _y_pred_up\n",
    "            },\n",
    "        title=strategy_name[_strat], \n",
    "        ax=axs[_i]\n",
    "        )\n",
    "    \n",
    "plt.savefig('output/prediction-intervals-timeseries-problem-with-change-point.png', dpi=200)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Marginal coverage on 24h rolling windows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE: int = 24  # in hours \n",
    "rolling_coverage = {_k: [] for _k in strategy_name.keys()}\n",
    "\n",
    "for i in range(WINDOW_SIZE, len(y_test)):\n",
    "    for _strat in strategy_name.keys():\n",
    "        rolling_coverage[_strat].append(\n",
    "            validate.coverage(\n",
    "                {_strat: int_pred[_strat][i-WINDOW_SIZE:i, :, :]}, \n",
    "                y_test[i-WINDOW_SIZE:i], silent=True)[_strat])\n",
    "\n",
    "visualize.rolling_coverage(\n",
    "    rolling_coverage, ts_problem.test_df[WINDOW_SIZE:].index, \n",
    "    WINDOW_SIZE, save_path='output/rolling-coverage-with-change-point.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Average goodness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_figs = len(base_model)\n",
    "fig, axs = plt.subplots(nrows=n_figs, figsize=(10, 5 * n_figs));\n",
    "\n",
    "for _i, _strat in enumerate(strategy_name.keys()):\n",
    "    # Predicted intervals\n",
    "    _y_pred_low = int_pred[_strat][:, 0, 0].ravel()\n",
    "    _y_pred_up = int_pred[_strat][:, 1, 0].ravel()\n",
    "\n",
    "    # Visualize\n",
    "    axs[_i] = visualize.goodness(\n",
    "        y_test, y_pred[_strat],\n",
    "        _y_pred_low, \n",
    "        _y_pred_up,\n",
    "        coverages[_strat],\n",
    "        widths[_strat],\n",
    "        rmse[_strat],\n",
    "        cwc[_strat],\n",
    "        ax=axs[_i],\n",
    "        fading_with_lead_time=True if _strat == 'EnbPI_nP' else False,\n",
    "        xy=(2.7, 6.5),\n",
    "        xytext=(2.7, 5.95),\n",
    "        subsample=0.5,\n",
    "        title=strategy_name[_strat], \n",
    "    )\n",
    "plt.savefig('output/average-goodness-timeseries-problem-with-change-point.png', dpi=200)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Marginal vs. conditional coverage: **width occurrence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_figs = len(base_model)\n",
    "fig, axs = plt.subplots(nrows=n_figs, figsize=(10, 5 * n_figs));\n",
    "\n",
    "_x_max = (1+1e-3) * np.max([np.abs(int_pred[_strat][:, 0, 0] - int_pred[_strat][:, 1, 0]) for _strat in int_pred.keys()]) \n",
    "_x_min = np.min([np.abs(int_pred[_strat][:, 0, 0] - int_pred[_strat][:, 1, 0]) for _strat in int_pred.keys()])\n",
    "\n",
    "for _i, _strat in enumerate(base_model.keys()):\n",
    "    # Visualize\n",
    "    axs[_i] = visualize.width_size_occurrence(\n",
    "        int_pred[_strat],\n",
    "        # train_intervals=int_pred_train[_strat],\n",
    "        num_bins=5,\n",
    "        ax=axs[_i],\n",
    "        x_lim=[_x_min, _x_max],\n",
    "        title=strategy_name[_strat], \n",
    "    )\n",
    "plt.savefig('output/width-occurrence-timeseries-problem-with-change-point.png', dpi=200)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Marginal vs. conditional coverage: **coverage vs. width**\n",
    "    - Since `EnbPI_nP` provides constant-width intervals, its plot is omitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_BINS: int = 10\n",
    "_strategies = [_k for _k in base_model.keys() if _k != 'EnbPI_nP']\n",
    "\n",
    "n_figs = len(_strategies)\n",
    "fig, axs = plt.subplots(nrows=n_figs, figsize=(10, 5 * n_figs));\n",
    "if not hasattr(axs, '__len__'):  # because nrows=1\n",
    "    axs = [axs]\n",
    "    \n",
    "cond_coverages: dict = validate.cond_coverage(int_pred, y_test, num_bins=NUM_BINS)\n",
    "hsic_coefficients: dict = validate.hsic_coefficient(int_pred, y_test) \n",
    "\n",
    "for _i, _strat in enumerate(_strategies):\n",
    "    axs[_i] = visualize.coverage_by_width(\n",
    "        y_test, int_pred[_strat], MISCOVERAGE,\n",
    "        cond_coverages[_strat],\n",
    "        hsic_coefficients[_strat],\n",
    "        num_bins=NUM_BINS,\n",
    "        ax=axs[_i],\n",
    "        title=strategy_name[_strat], \n",
    "    )\n",
    "plt.savefig('output/coverage-vs-width-timeseries-problem-with-change-point.png', dpi=200)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Metrics table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = visualize.dicts_to_dataframe(\n",
    "    {'Coverage': coverages, 'Int. width': widths, \n",
    "     'RMSE': rmse, 'CWC': cwc, \n",
    "     'SSC score': cond_coverages, \n",
    "     'HSIC': hsic_coefficients})\n",
    "\n",
    "visualize.dataframe_to_png(metrics_df, 'output/metrics-table-timeseries-problem-with-change-point.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Coverage sougth for different values of $\\alpha$**: \n",
    "    - First, the 5 folds' data is shown\n",
    "    - Then, the 5-fold CV is executed and its results plotted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.visualize_ts_K_folds(5, with_change_point=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_RESULTS: str = 'output/coverage-cv-results-with-change-point.pickle'\n",
    "MISCOVERAGES_LIST: np.ndarray = np.round(np.linspace(0.01, 0.20, 5), 2)\n",
    "N_FOLDS: int = 5\n",
    "\n",
    "if os.path.isfile(CV_RESULTS):\n",
    "    logger.info(\"Opening serialized CV results\")\n",
    "    with open(CV_RESULTS, 'rb') as f:\n",
    "        coverages_dict = pickle.load(f)\n",
    "else:\n",
    "    coverages_dict = {}\n",
    "    logger.info(\"Training the different strategis and for different alpha values\")\n",
    "    logger.debug(4 * \" \" + \"This may take a while (AROUND 30' MINUTES)\")\n",
    "\n",
    "    for _strat, base_estimator in base_model.items():\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            coverages_dict[_strat] = cv.ts_coverage_in_function_of_alpha(\n",
    "                MISCOVERAGES_LIST, best_model_params, _strat, \n",
    "                with_change_point=True, silent=True, K=N_FOLDS)\n",
    "    \n",
    "    logger.info(\"Serializing obtained CV results\")\n",
    "    with open(CV_RESULTS, 'wb') as f:\n",
    "        pickle.dump(coverages_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_figs = len(base_model)\n",
    "fig, axs = plt.subplots(nrows=n_figs, figsize=(10, 5 * n_figs));\n",
    "\n",
    "for _i, _strat in enumerate(base_model.keys()):\n",
    "    axs[_i] = visualize.coverage_by_alpha(\n",
    "        coverages_dict[_strat], \n",
    "        MISCOVERAGES_LIST, \n",
    "        strategy_name[_strat],\n",
    "        ax=axs[_i]\n",
    "        )\n",
    "plt.savefig('output/coverage-vs-alpha-timeseries-problem-with-change-problem.png', dpi=200)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ConformalPrediction",
   "language": "python",
   "name": "conformalprediction"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
